{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample 2: Scopes & Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Originally by: Kaspar Sakmann (https://github.com/ksakmann)\n",
    "\n",
    "A Convolutional Network implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "old_v = tf.logging.get_verbosity()\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "tf.logging.set_verbosity(old_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "with tf.name_scope(\"input_var\"):\n",
    "    x = tf.placeholder(tf.float32, [None, n_input])\n",
    "with tf.name_scope(\"output_var\"):\n",
    "    y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "with tf.name_scope(\"keep_prob\"):\n",
    "    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, name, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    with tf.name_scope(name):\n",
    "        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "        x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, name, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    with tf.name_scope(name):\n",
    "        pool = tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "    return pool\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    with tf.name_scope(\"input_img\"):\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['weights_conv1'], biases['bias_conv1'], \"conv1\")\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1,\"pooling_1\", k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['weights_conv2'], biases['bias_conv2'], \"conv2\")\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, \"pooling_2\", k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        fc1 = tf.reshape(conv2, [-1, weights['weights_fully'].get_shape().as_list()[0]])\n",
    "        fc1 = tf.add(tf.matmul(fc1, weights['weights_fully']), biases['bias_fully'])\n",
    "        fc1 = tf.nn.relu(fc1)\n",
    "        # Apply Dropout\n",
    "        fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    with tf.name_scope(\"class_prediction\"):\n",
    "        out = tf.add(tf.matmul(fc1, weights['weights_out']), biases['bias_out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "log_dir = \"/tmp/tensorflow_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope(name):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store layers weight & bias\n",
    "with tf.name_scope(\"weights\"):\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 32 outputs\n",
    "        'weights_conv1': tf.Variable(tf.random_normal([5, 5, 1, 32]), name=\"weights_conv1\"),\n",
    "        # 5x5 conv, 32 inputs, 64 outputs\n",
    "        'weights_conv2': tf.Variable(tf.random_normal([5, 5, 32, 64]), name=\"weights_conv2\"),\n",
    "        # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "        'weights_fully': tf.Variable(tf.random_normal([7*7*64, 1024]), name=\"weights_fully\"),\n",
    "        # 1024 inputs, 10 outputs (class prediction)\n",
    "        'weights_out': tf.Variable(tf.random_normal([1024, n_classes], name=\"weights_out\"))\n",
    "    }\n",
    "\n",
    "with tf.name_scope(\"biases\"):\n",
    "    biases = {\n",
    "        'bias_conv1': tf.Variable(tf.random_normal([32]), name=\"bias_conv1\"),\n",
    "        'bias_conv2': tf.Variable(tf.random_normal([64]), name=\"bias_conv2\"),\n",
    "        'bias_fully': tf.Variable(tf.random_normal([1024]), name=\"bias_fully\"),\n",
    "        'bias_out': tf.Variable(tf.random_normal([n_classes]), name=\"bias_out\")\n",
    "    }\n",
    "\n",
    "#set advanced variable summaries logging\n",
    "for lbl, weight in weights.items():\n",
    "    variable_summaries(weight, lbl)\n",
    "for lbl, bias in biases.items():\n",
    "    variable_summaries(bias, lbl)\n",
    "\n",
    "# Construct model\n",
    "with tf.name_scope(\"model\"):\n",
    "    pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "with tf.name_scope(\"learning\"):\n",
    "    # Define loss and optimizer\n",
    "    with tf.name_scope(\"cost\"):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    # Evaluate model\n",
    "    with tf.name_scope(\"correct_pred\"):\n",
    "        correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Create a summary to monitor cost tensor\n",
    "tf.summary.scalar(\"loss\", cost)\n",
    "# Create a summary to monitor accuracy tensor\n",
    "tf.summary.scalar(\"accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 39374.398438, Training Accuracy= 0.13281\n",
      "Iter 2560, Minibatch Loss= 17925.714844, Training Accuracy= 0.28906\n",
      "Iter 3840, Minibatch Loss= 10131.162109, Training Accuracy= 0.59375\n",
      "Iter 5120, Minibatch Loss= 8189.510254, Training Accuracy= 0.70312\n",
      "Iter 6400, Minibatch Loss= 6857.375977, Training Accuracy= 0.71094\n",
      "Iter 7680, Minibatch Loss= 4600.090820, Training Accuracy= 0.76562\n",
      "Iter 8960, Minibatch Loss= 2898.402832, Training Accuracy= 0.82812\n",
      "Iter 10240, Minibatch Loss= 3878.527344, Training Accuracy= 0.81250\n",
      "Iter 11520, Minibatch Loss= 3515.367432, Training Accuracy= 0.85156\n",
      "Iter 12800, Minibatch Loss= 1982.174805, Training Accuracy= 0.86719\n",
      "Iter 14080, Minibatch Loss= 2751.615723, Training Accuracy= 0.85938\n",
      "Iter 15360, Minibatch Loss= 999.437744, Training Accuracy= 0.94531\n",
      "Iter 16640, Minibatch Loss= 2578.463867, Training Accuracy= 0.83594\n",
      "Iter 17920, Minibatch Loss= 1406.256836, Training Accuracy= 0.89844\n",
      "Iter 19200, Minibatch Loss= 943.690979, Training Accuracy= 0.90625\n",
      "Iter 20480, Minibatch Loss= 317.180298, Training Accuracy= 0.97656\n",
      "Iter 21760, Minibatch Loss= 1321.562744, Training Accuracy= 0.95312\n",
      "Iter 23040, Minibatch Loss= 1916.636963, Training Accuracy= 0.90625\n",
      "Iter 24320, Minibatch Loss= 1660.166504, Training Accuracy= 0.89062\n",
      "Iter 25600, Minibatch Loss= 1467.482422, Training Accuracy= 0.88281\n",
      "Iter 26880, Minibatch Loss= 1996.344727, Training Accuracy= 0.87500\n",
      "Iter 28160, Minibatch Loss= 2060.842285, Training Accuracy= 0.88281\n",
      "Iter 29440, Minibatch Loss= 1748.858276, Training Accuracy= 0.89844\n",
      "Iter 30720, Minibatch Loss= 2359.870605, Training Accuracy= 0.89062\n",
      "Iter 32000, Minibatch Loss= 973.124268, Training Accuracy= 0.93750\n",
      "Iter 33280, Minibatch Loss= 898.245239, Training Accuracy= 0.92969\n",
      "Iter 34560, Minibatch Loss= 771.355103, Training Accuracy= 0.93750\n",
      "Iter 35840, Minibatch Loss= 1371.873291, Training Accuracy= 0.91406\n",
      "Iter 37120, Minibatch Loss= 984.730957, Training Accuracy= 0.92969\n",
      "Iter 38400, Minibatch Loss= 550.216675, Training Accuracy= 0.96094\n",
      "Iter 39680, Minibatch Loss= 1571.065430, Training Accuracy= 0.87500\n",
      "Iter 40960, Minibatch Loss= 1446.669434, Training Accuracy= 0.94531\n",
      "Iter 42240, Minibatch Loss= 1434.172852, Training Accuracy= 0.91406\n",
      "Iter 43520, Minibatch Loss= 1153.410889, Training Accuracy= 0.96875\n",
      "Iter 44800, Minibatch Loss= 828.015076, Training Accuracy= 0.95312\n",
      "Iter 46080, Minibatch Loss= 505.420593, Training Accuracy= 0.96094\n",
      "Iter 47360, Minibatch Loss= 882.380005, Training Accuracy= 0.94531\n",
      "Iter 48640, Minibatch Loss= 249.653946, Training Accuracy= 0.96094\n",
      "Iter 49920, Minibatch Loss= 1042.221436, Training Accuracy= 0.95312\n",
      "Iter 51200, Minibatch Loss= 826.091003, Training Accuracy= 0.92969\n",
      "Iter 52480, Minibatch Loss= 89.462166, Training Accuracy= 0.97656\n",
      "Iter 53760, Minibatch Loss= 468.762146, Training Accuracy= 0.95312\n",
      "Iter 55040, Minibatch Loss= 1473.862061, Training Accuracy= 0.90625\n",
      "Iter 56320, Minibatch Loss= 885.837585, Training Accuracy= 0.93750\n",
      "Iter 57600, Minibatch Loss= 957.934387, Training Accuracy= 0.94531\n",
      "Iter 58880, Minibatch Loss= 186.217941, Training Accuracy= 0.97656\n",
      "Iter 60160, Minibatch Loss= 317.864624, Training Accuracy= 0.97656\n",
      "Iter 61440, Minibatch Loss= 200.236908, Training Accuracy= 0.98438\n",
      "Iter 62720, Minibatch Loss= 233.543823, Training Accuracy= 0.97656\n",
      "Iter 64000, Minibatch Loss= 966.633911, Training Accuracy= 0.93750\n",
      "Iter 65280, Minibatch Loss= 387.533325, Training Accuracy= 0.96094\n",
      "Iter 66560, Minibatch Loss= 1218.655762, Training Accuracy= 0.95312\n",
      "Iter 67840, Minibatch Loss= 1098.479248, Training Accuracy= 0.93750\n",
      "Iter 69120, Minibatch Loss= 248.415833, Training Accuracy= 0.96094\n",
      "Iter 70400, Minibatch Loss= 73.461395, Training Accuracy= 0.97656\n",
      "Iter 71680, Minibatch Loss= 461.428436, Training Accuracy= 0.93750\n",
      "Iter 72960, Minibatch Loss= 528.111511, Training Accuracy= 0.96094\n",
      "Iter 74240, Minibatch Loss= 477.962616, Training Accuracy= 0.96094\n",
      "Iter 75520, Minibatch Loss= 755.659485, Training Accuracy= 0.95312\n",
      "Iter 76800, Minibatch Loss= 464.495514, Training Accuracy= 0.95312\n",
      "Iter 78080, Minibatch Loss= 30.057877, Training Accuracy= 0.98438\n",
      "Iter 79360, Minibatch Loss= 1016.084717, Training Accuracy= 0.95312\n",
      "Iter 80640, Minibatch Loss= 254.522919, Training Accuracy= 0.96094\n",
      "Iter 81920, Minibatch Loss= 688.026550, Training Accuracy= 0.94531\n",
      "Iter 83200, Minibatch Loss= 721.342773, Training Accuracy= 0.93750\n",
      "Iter 84480, Minibatch Loss= 466.729309, Training Accuracy= 0.96094\n",
      "Iter 85760, Minibatch Loss= 264.841003, Training Accuracy= 0.96875\n",
      "Iter 87040, Minibatch Loss= 648.547668, Training Accuracy= 0.94531\n",
      "Iter 88320, Minibatch Loss= 410.692535, Training Accuracy= 0.97656\n",
      "Iter 89600, Minibatch Loss= 857.230957, Training Accuracy= 0.94531\n",
      "Iter 90880, Minibatch Loss= 163.281158, Training Accuracy= 0.97656\n",
      "Iter 92160, Minibatch Loss= 90.905655, Training Accuracy= 0.97656\n",
      "Iter 93440, Minibatch Loss= 164.440277, Training Accuracy= 0.97656\n",
      "Iter 94720, Minibatch Loss= 602.490967, Training Accuracy= 0.96094\n",
      "Iter 96000, Minibatch Loss= 144.272949, Training Accuracy= 0.97656\n",
      "Iter 97280, Minibatch Loss= 453.096558, Training Accuracy= 0.92188\n",
      "Iter 98560, Minibatch Loss= 503.763733, Training Accuracy= 0.93750\n",
      "Iter 99840, Minibatch Loss= 581.273071, Training Accuracy= 0.96094\n",
      "Iter 101120, Minibatch Loss= 330.231262, Training Accuracy= 0.96094\n",
      "Iter 102400, Minibatch Loss= 329.794800, Training Accuracy= 0.96094\n",
      "Iter 103680, Minibatch Loss= 387.496582, Training Accuracy= 0.96875\n",
      "Iter 104960, Minibatch Loss= 550.462280, Training Accuracy= 0.93750\n",
      "Iter 106240, Minibatch Loss= 363.176392, Training Accuracy= 0.96094\n",
      "Iter 107520, Minibatch Loss= 817.211121, Training Accuracy= 0.92188\n",
      "Iter 108800, Minibatch Loss= 366.707520, Training Accuracy= 0.96094\n",
      "Iter 110080, Minibatch Loss= 48.111298, Training Accuracy= 0.99219\n",
      "Iter 111360, Minibatch Loss= 355.040802, Training Accuracy= 0.96094\n",
      "Iter 112640, Minibatch Loss= 151.959274, Training Accuracy= 0.97656\n",
      "Iter 113920, Minibatch Loss= 32.509888, Training Accuracy= 0.99219\n",
      "Iter 115200, Minibatch Loss= 339.492676, Training Accuracy= 0.96094\n",
      "Iter 116480, Minibatch Loss= 356.040527, Training Accuracy= 0.96094\n",
      "Iter 117760, Minibatch Loss= 175.844406, Training Accuracy= 0.96094\n",
      "Iter 119040, Minibatch Loss= 98.258446, Training Accuracy= 0.97656\n",
      "Iter 120320, Minibatch Loss= 764.254822, Training Accuracy= 0.94531\n",
      "Iter 121600, Minibatch Loss= 170.522568, Training Accuracy= 0.97656\n",
      "Iter 122880, Minibatch Loss= 637.786560, Training Accuracy= 0.95312\n",
      "Iter 124160, Minibatch Loss= 86.339790, Training Accuracy= 0.98438\n",
      "Iter 125440, Minibatch Loss= 138.689484, Training Accuracy= 0.98438\n",
      "Iter 126720, Minibatch Loss= 252.127853, Training Accuracy= 0.96094\n",
      "Iter 128000, Minibatch Loss= 140.676331, Training Accuracy= 0.98438\n",
      "Iter 129280, Minibatch Loss= 181.136841, Training Accuracy= 0.97656\n",
      "Iter 130560, Minibatch Loss= 525.908569, Training Accuracy= 0.95312\n",
      "Iter 131840, Minibatch Loss= 658.352356, Training Accuracy= 0.93750\n",
      "Iter 133120, Minibatch Loss= 291.001495, Training Accuracy= 0.97656\n",
      "Iter 134400, Minibatch Loss= 423.405396, Training Accuracy= 0.96094\n",
      "Iter 135680, Minibatch Loss= 249.244873, Training Accuracy= 0.96875\n",
      "Iter 136960, Minibatch Loss= 189.477295, Training Accuracy= 0.96875\n",
      "Iter 138240, Minibatch Loss= 66.890091, Training Accuracy= 0.98438\n",
      "Iter 139520, Minibatch Loss= 460.111084, Training Accuracy= 0.97656\n",
      "Iter 140800, Minibatch Loss= 126.468185, Training Accuracy= 0.98438\n",
      "Iter 142080, Minibatch Loss= 525.443237, Training Accuracy= 0.92969\n",
      "Iter 143360, Minibatch Loss= 394.509888, Training Accuracy= 0.92969\n",
      "Iter 144640, Minibatch Loss= 350.573456, Training Accuracy= 0.96094\n",
      "Iter 145920, Minibatch Loss= 196.211105, Training Accuracy= 0.96094\n",
      "Iter 147200, Minibatch Loss= 333.694397, Training Accuracy= 0.96094\n",
      "Iter 148480, Minibatch Loss= 148.119705, Training Accuracy= 0.98438\n",
      "Iter 149760, Minibatch Loss= 403.300659, Training Accuracy= 0.96875\n",
      "Iter 151040, Minibatch Loss= 205.689713, Training Accuracy= 0.96094\n",
      "Iter 152320, Minibatch Loss= 122.407822, Training Accuracy= 0.99219\n",
      "Iter 153600, Minibatch Loss= 196.772247, Training Accuracy= 0.96875\n",
      "Iter 154880, Minibatch Loss= 245.744370, Training Accuracy= 0.94531\n",
      "Iter 156160, Minibatch Loss= 512.947815, Training Accuracy= 0.95312\n",
      "Iter 157440, Minibatch Loss= 79.315781, Training Accuracy= 0.97656\n",
      "Iter 158720, Minibatch Loss= 163.104263, Training Accuracy= 0.96875\n",
      "Iter 160000, Minibatch Loss= 235.774353, Training Accuracy= 0.96094\n",
      "Iter 161280, Minibatch Loss= 738.084961, Training Accuracy= 0.94531\n",
      "Iter 162560, Minibatch Loss= 296.378937, Training Accuracy= 0.96094\n",
      "Iter 163840, Minibatch Loss= 381.579254, Training Accuracy= 0.96875\n",
      "Iter 165120, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 166400, Minibatch Loss= 453.225830, Training Accuracy= 0.96094\n",
      "Iter 167680, Minibatch Loss= 87.162613, Training Accuracy= 0.98438\n",
      "Iter 168960, Minibatch Loss= 0.000060, Training Accuracy= 1.00000\n",
      "Iter 170240, Minibatch Loss= 159.012634, Training Accuracy= 0.96875\n",
      "Iter 171520, Minibatch Loss= 247.802109, Training Accuracy= 0.96094\n",
      "Iter 172800, Minibatch Loss= 184.648605, Training Accuracy= 0.97656\n",
      "Iter 174080, Minibatch Loss= 133.552338, Training Accuracy= 0.98438\n",
      "Iter 175360, Minibatch Loss= 0.000000, Training Accuracy= 1.00000\n",
      "Iter 176640, Minibatch Loss= 451.955933, Training Accuracy= 0.96875\n",
      "Iter 177920, Minibatch Loss= 423.978790, Training Accuracy= 0.94531\n",
      "Iter 179200, Minibatch Loss= 94.276039, Training Accuracy= 0.96875\n",
      "Iter 180480, Minibatch Loss= 122.906097, Training Accuracy= 0.99219\n",
      "Iter 181760, Minibatch Loss= 222.686508, Training Accuracy= 0.96875\n",
      "Iter 183040, Minibatch Loss= 295.730408, Training Accuracy= 0.96875\n",
      "Iter 184320, Minibatch Loss= 106.190552, Training Accuracy= 0.96094\n",
      "Iter 185600, Minibatch Loss= 152.681671, Training Accuracy= 0.96094\n",
      "Iter 186880, Minibatch Loss= 38.980324, Training Accuracy= 0.99219\n",
      "Iter 188160, Minibatch Loss= 177.193787, Training Accuracy= 0.96094\n",
      "Iter 189440, Minibatch Loss= 69.664536, Training Accuracy= 0.97656\n",
      "Iter 190720, Minibatch Loss= 97.529526, Training Accuracy= 0.97656\n",
      "Iter 192000, Minibatch Loss= 44.437485, Training Accuracy= 0.99219\n",
      "Iter 193280, Minibatch Loss= 34.095695, Training Accuracy= 0.99219\n",
      "Iter 194560, Minibatch Loss= 407.247314, Training Accuracy= 0.96094\n",
      "Iter 195840, Minibatch Loss= 242.242828, Training Accuracy= 0.96875\n",
      "Iter 197120, Minibatch Loss= 17.638931, Training Accuracy= 0.98438\n",
      "Iter 198400, Minibatch Loss= 203.964203, Training Accuracy= 0.96875\n",
      "Iter 199680, Minibatch Loss= 250.833405, Training Accuracy= 0.95312\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.9921875\n",
      "Run the command line:\n",
      "--> tensorboard --logdir=/tmp/tensorflow_logs \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()    \n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        \n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        _, summary = sess.run([optimizer, merged ], feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        \n",
    "        train_writer.add_summary(summary, step*batch_size)\n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print (\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print (\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print (\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "                                      y: mnist.test.labels[:256],\n",
    "                                      keep_prob: 1.}))\n",
    "    print (\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=\"+log_dir+\" \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:rllab3]",
   "language": "python",
   "name": "conda-env-rllab3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
